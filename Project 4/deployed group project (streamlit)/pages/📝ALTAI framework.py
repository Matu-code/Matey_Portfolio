import streamlit as st


import streamlit as st


st.title ( ":green[ATLAI framework]")


tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs(["Intro","Requirement 1","Requirement 2", "Requirement 3 ", 
"Requirement 4","Requirement 5"
,"Requirement 6","Requirement 7"])




with tab1:
    st.markdown("## *:green[Ethical and Legal framework ]*")
    st.markdown(" ### *While working on this project we are of course keeping the legal and ethical framework in mind. This includes answering the DEDA and AlTAI frameworks in this document as well. The checklists for the same have been answered below. I will also provide a link to the download path for the whole document here:.*")





with tab2:
    st.subheader("*:violet[REQUIREMENT #1 Human Agency and Oversight]*")
    st.markdown(":red[1. Does the AI system potentially negatively discriminate against people on the basis of any of the following grounds (non-exhaustively): sex, race, color, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, birth, disability, age or sexual orientation?] ")
    st.markdown("When making a model like this, you always keep these things in mind. It is, however , almost impossible. There are sadly almost always innate biases when working with machine learning models. Moreover, we are building a model that uses data that might be sensitive and we make an effort into securing the data in a proper manner.")
    st.markdown(":red[2. Does the AI system respect the rights of the child, for example with respect to child protection and taking the child’s best interests into account? Have you put in place processes to address and rectify potential harm to children by the AI system? Have you put in place processes to test and monitor for potential harm to children during the development, deployment and use phases of the AI system?]")
    st.markdown("We are making our model with the child’s best interest at heart, and making sure to protect not only the child but everyone’s best interests with our model.")
    st.markdown(":red[3. Does the AI system protect personal data relating to individuals in line with GDPR?16 Have you put in place processes to assess in detail the need for a data protection impact assessment, including an assessment of the necessity and proportionality of the processing operations in relation to their purpose, with respect to the development, deployment and use phases of the AI system? Have you put in place measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data with respect to the development, deployment and use phases of the AI system? See the section on Privacy and Data Governance in this Assessment List, and available guidance from the European Data Protection Supervisor?]")
    st.markdown("We keep in mind the GDPR. We do this by making it mandatory to log in to the website. This way we lock any data that might unintentionally harm anyone behind a password. Furthermore, this log-in code will only be provided to the “product owner” and the municipality of Breda. We do this to keep our model secure.")
    st.markdown(":red[4. Does the AI system respect freedom of expression, information, and/or freedom of assembly and association? Have you put in place processes to test and monitor for potential infringement on freedom of expression and information, and/or freedom of assembly and association, during the development, deployment and use phases of the AI system? Have you put in place processes to address and rectify potential infringement on freedom of expression and information, and/or freedom of assembly and association, in the AI system?]")
    st.markdown("Yes, Our AI system respects freedom of expression, information, and/or freedom of assembly and association. We are making a very basic model that does not infringe on any of these basic rights.")
    st.markdown(":orange[* Is the AI system designed to interact, guide or take decisions by human end-users that affect humans or society?]")
    st.markdown("No, our model only gives outputs based on the information it has been given. Then it predicts how much the other variable needs to be in order to keep the correlation intact. What is done with these predictions is up to the end user. Our model predicts the crime rate for the asked features, what is done with that information is down to the end user.")
    st.markdown(":red[5. Could the AI system generate confusion for some or all end-users or subjects on whether a decision, content, advice or outcome is the result of an algorithmic decision?] ")
    st.markdown("No. Our AI system gives you one value at the end of its prediction: The predicted level of crime for specified features.")
    st.markdown(":red[6. Are end-users or other subjects adequately made aware that a decision, content, advice or outcome is the result of an algorithmic decision?] ")
    st.markdown("Yes, in our dashboard the inner workings of the model will be explained and made easy to understand for the end user. Furthermore, we document all the code in this document as well. This will make it easier for the end user to understand and use our model.")
    st.markdown(":red[7. Could the AI system generate confusion for some or all end-users or subjects on whether they are interacting with a human or AI system?] ")
    st.markdown("No In our dashboard, we will make it very clear that they are working with a machine-learning model. We do this by only making it give you outputs based on what basic information you give it. This will make it so there is no other interaction that could resemble that of a human.")
    st.markdown(":red[8. Are end-users or subjects informed that they are interacting with an AI system?] ")
    st.markdown("Yes. The path that leads towards the model outputs will be named in such a way that it’s clear that the end user is interacting with a machine learning model.")
    st.markdown(":orange* Could the AI system affect human autonomy by generating over-reliance by end-users?] ")
    st.markdown("No. Our model is only a tool that will be very useful in one singular situation. This makes it so it has no additional value besides that one specific situation.")
    st.markdown(":red[9. Did you put in place procedures to avoid end-users over-rely on the AI system?] ")
    st.markdown("Yes. As stated before, our model has only a simple goal and that goal is just an output of numbers. It will be very difficult for end-users to become over-reliant on the model.")
    st.markdown(":orange[* Could the AI system affect human autonomy by interfering with the end-user's decision-making process in any other unintended and undesirable way?]")
    st.markdown("No. As stated before, our model functions to assist the end-user by giving them suggestions for how we might need to improve our police force with the number of officers.")
    st.markdown(":orange[* Did you put in place any procedure to avoid that the AI system from inadvertently affecting human autonomy?] ")
    st.markdown("The way we avoid affecting human autonomy is by way of not making our model too complex. We have it calculate a simple metric based on the model’s inputs and have that as our output. These outputs are merely a suggestion.")
    st.markdown(":red[10. Does the AI system simulate social interaction with or between end-users or subjects?]")
    st.markdown("Not necessarily. The model calculates a simple metric based on the info it has been given and then gives an output.")
    st.markdown(":red[11.Does the AI system risk creating human attachment, stimulating addictive behavior, or manipulating user behavior? Depending on which risks are possible or likely, please answer the questions below:]")
    st.markdown("Our model does not create human attachment or anything in that regard since it just calculates a simple metric based on the given inputs and has it as its output.")
    st.markdown(":orange[* Did you take measures to deal with possible negative consequences for end-users or subjects if they develop a disproportionate attachment to the AI System?]")
    st.markdown("Our model does not invoke such things since it’s so simple and does not resemble any kind of human interaction.")                
    st.markdown(":orange[* Did you take measures to minimize the risk of addiction?]")
    st.markdown("Our model does not invoke such things since it’s so simple and does not resemble any kind of human interaction.")
    st.markdown(":orange[* Did you take measures to mitigate the risk of manipulation?]")
    st.markdown("Our model does not invoke such things since it’s so simple and does not resemble any kind of human interaction")
    st.markdown(":red[12. Have the humans (human-in-the-loop, human-on-the-loop, human-in-command) been given specific training on how to exercise oversight?]")
    st.markdown("Yes. We are currently doing a study on it. We have been well-informed on how to make and maintain these models.")
    st.markdown(":orange[* Did you establish any detection and response mechanisms for undesirable adverse effects of the AI system for the end-user or subject?] ")
    st.markdown("We are overseeing every moment the model is running, we do this to catch any undesirable effects and patch them out immediately so we can deliver a proper product.")
    st.markdown(":orange[* Did you ensure a ‘stop button’ or procedure to safely abort an operation when needed?]")
    st.markdown("Yes. There is a built-in stop button in the environments we use.")
    st.markdown(":orange[ * Did you take any specific oversight and control measures to reflect the self-learning or autonomous nature of the AI system?]")
    st.markdown("No. The model is not complicated enough to require this.")

with tab3:
    st.subheader("*:violet[REQUIREMENT #2 Technical Robustness and Safet]*")
    st.markdown(":blue[1. Could the AI system have adversarial, critical, or damaging effects (e.g. to human or societal safety) in case of risks or threats such as design or technical faults, defects, outages, attacks, misuse, inappropriate or malicious use?]")
    st.markdown("No. it will not. It’s a very simple model")
    st.markdown(":blue[2. Is the AI system certified for cybersecurity (e.g. the certification scheme created by the Cybersecurity Act in Europe) or is it compliant with specific security standards? • How exposed is the AI system to cyber-attacks?] ")
    st.markdown("No. it doesn’t need to be.")
    st.markdown(":blue[3. Did you assess potential forms of attacks to which the AI system could be vulnerable?] ")
    st.markdown("No. since it’s not that advanced.")
    st.markdown(":blue[4. Did you consider different types of vulnerabilities and potential entry points for attacks such as:] ")
    st.markdown(":violet[* Data poisoning (i.e. manipulation of training dat*;]  ")
    st.markdown(":violet[* Model evasion (i.e. classifying the data according to the attacker's will);]  ")
    st.markdown(":violet[* Model inversion (i.e. inferring the model parameters)] ")
    st.markdown("Yes. but it does not apply to our model.")
    st.markdown(":violet[* Did you put measures in place to ensure the integrity, robustness, and overall security of the AI system against potential attacks over its lifecycle?] ")
    st.markdown("No, our model is not complex enough to make it necessary.")
    st.markdown(":violet[* Did you red-team/pentest the system?] ")
    st.markdown(":violet[* Did you inform end-users of the duration of security coverage and updates?]")
    st.markdown("Not applicable.")
    st.markdown(":blue[5. What length is the expected timeframe within which you provide security updates for the AI system?]")
    st.markdown("Not applicable.")
    st.markdown("------------")
    st.subheader("*:green[General Safety]*")
    st.markdown(":blue[1. Did you define risks, risk metrics, and risk levels of the AI system in each specific use case?] ")
    st.markdown("Yes. We have brainstormed and looked at our potential risks.")
    st.markdown(":blue[2. Did you put in place a process to continuously measure and assess risks?] ")
    st.markdown("Yes. The model gives constant output, making it unnecessary.")
    st.markdown(":blue[3. Did you inform end-users and subjects of existing or potential risks?]")
    st.markdown("Yes, we will be informing the end-users how to use our model. But the model comes without risks seeing as it’s not a complex model.")
    st.markdown(":blue[4. Did you identify the possible threats to the AI system (design faults, technical faults, environmental threats) and the possible consequences? ] ")
    st.markdown("Yes. We have brainstormed and looked at our potential risks. ")
    st.markdown(":blue[5. Did you assess the risk of possible malicious use, misuse, or inappropriate use of the AI system?]")
    st.markdown("Yes. We have brainstormed and looked at our potential risks")
    st.markdown(":blue[6. Did you define safety criticality levels (e.g. related to human integrity) of the possible consequences of faults or misuse of the AI system?] ")
    st.markdown("Yes. We have brainstormed and looked at our potential risks.")
    st.markdown(":blue[7. Did you assess the dependency of a critical AI system’s decisions on its stable and reliable behavior?] ")
    st.markdown("Yes. We have brainstormed and looked at our potential risks.")
    st.markdown(":blue[8. Did you plan fault tolerance via, e.g. a duplicated system or another parallel system (AI-based or ‘conventional’)?] ")
    st.markdown("No. We did not, seeing as our model is not complex enough")
    st.markdown(":blue[9. Did you develop a mechanism to evaluate when the AI system has been changed to merit a new review of its technical robustness and safety?] ")
    st.markdown("We are evaluating our model constantly throughout the project lifecycle. Updating it constantly.")
    st.markdown("------------")
    st.subheader("*:green[Accuracy]*")
    st.markdown(":blue[1. Could a low level of accuracy of the AI system result in critical, adversarial, or damaging consequences?]")
    st.markdown("No, it can not.")
    st.markdown(":blue[2. Did you put in place measures to ensure that the data (including training dat* used to develop the AI system is up-to-date, of high quality, complete, and representative of the environment in the system will be deployed? ]") 
    st.markdown("Yes. We have, seeing as we are using data from the municipality of Breda. Furthermore, we clean all the data before using it.")
    st.markdown(":blue[3. Did you put in place a series of steps to monitor, and document the AI system’s accuracy? ]")
    st.markdown("Yes.")
    st.markdown(":blue[4. Did you consider whether the AI system's operation can invalidate the data or assumptions it was trained on, and how this might lead to adversarial effects?] ")
    st.markdown("Yes. This is why it needs to be fed new and updated information if it wants to stay operational.")
    st.markdown(":blue[5. Did you put processes in place to ensure that the level of accuracy of the AI system to be expected by end-users and/or subjects is properly communicated?]")
    st.markdown("Yes")
    st.markdown(":blue[6. Could the AI system cause critical, adversarial, or damaging consequences (e.g. pertaining to human safety) in case of low reliability and/or reproducibility? ]")
    st.markdown("No")
    st.markdown(" :blue[7. Did you put in place a well-defined process to monitor if the AI system is meeting the intended goals?]")
    st.markdown("Yes. We have been documenting all our progress when working with the model so far. We do this to see if the model has the capabilities to reach our end goal.")
    st.markdown(":blue[8. Did you test whether specific contexts or conditions need to be taken into account to ensure reproducibility?]")
    st.markdown("Yes. We are using datasets from different years to validate the reproducibility of our model.]")
    st.markdown(":blue[9. Did you put in place verification and validation methods and documentation (e.g. logging) to evaluate and ensure different aspects of the AI system’s reliability and reproducibility?]")
    st.markdown("Yes. We are logging everything we are working on in this document as a form of code documentation.")
    st.markdown(":blue[10. Did you clearly document and operationalize processes for the testing and verification of the reliability and reproducibility of the AI system? ]")
    st.markdown("Yes. We are logging everything we are working on in this document as a form of code documentation.")
    st.markdown(":blue[11. Did you define tested failsafe fallback plans to address AI system errors of whatever origin and put governance procedures in place to trigger them? ]")
    st.markdown("We do not need a fallback plan since our model isn’t advanced enough for that.")
    st.markdown(":blue[12. Did you put in place a proper procedure for handling the cases where the AI system yields results with a low confidence score? ]")
    st.markdown("Yes. We will be giving it different data to improve its confidence score.")
    st.markdown(":blue[13. Is your AI system using (online) continual learning?] ")
    st.markdown("No")
    st.markdown(":blue[14. Did you consider the potential negative consequences of the AI system learning novel or unusual methods to score well on its objective function?]")
    st.markdown("Our model does not have these learning methods so we do not need something like that.")
    st.markdown("*:green[Privacy]*")
    



with tab4:
    st.subheader("*:violet[REQUIREMENT #3 Privacy and Data Governance] ")
    st.subheader(" *:green[Privacy]*")
    st.markdown(":orange[1. Did you consider the impact of the AI system on the right to privacy, the right to physical, mental, and/or moral integrity, and the right to data protection?]")
    st.markdown("Yes. We always try to keep the legal and ethical framework in mind when working on our AI-centered project")
    st.markdown(":orange[2. Depending on the use case, did you establish mechanisms that allow flagging issues related to privacy concerning the AI system? Data Governance This subsection helps to self-assess the adherence of the AI system('s use) to various elements concerning data protection.]")
    st.markdown("Yes.")
    st.markdown("------------")
    st.subheader("*:green[Data Governance]*")
    st.markdown(":orange[1. Is your AI system being trained, or was it developed, by using or processing personal data (including special categories of personal dat*? ]")
    st.markdown("No")
    st.markdown(":orange[2. Did you implement the right to withdraw consent, the right to object, and the right to be forgotten into the development of the AI system? o Did you consider the privacy and data protection implications of data collected, generated, or processed over the course of the AI system's life cycle?]")
    st.markdown("Our project does not mention any personal data of any kind. This is not applicable.")
    st.markdown(":orange[3.  Did you implement the right to withdraw consent, the right to object, and the right to be forgotten into the development of the AI system? o Did you consider the privacy and data protection implications of data collected, generated, or processed over the course of the AI system's life cycle?] ")
    st.markdown("Our project does not mention any personal data of any kind. This is not applicable.")
    st.markdown(":orange[4. Did you consider the privacy and data protection implications of the AI system's non-personal training data or other processed non-personal data?]")
    st.markdown("yes")
    st.markdown(":orange[5. Did you align the AI system with relevant standards (e.g. ISO25, IEEE26) or widely adopted protocols for (daily) data management and governance?] ")
    st.markdown("Our project does not mention any personal data of any kind. This is not applicable")
    st.markdown("")
    
    
    
    
with tab5:  
    st.subheader("*:violet[REQUIREMENT #4 - Diversity, Non-Discrimination, And Fairness]*")
    st.subheader(" *:green[Traceability]*")
    st.markdown(":violet[1. Did you put in place measures that address the traceability of the AI system during its entire lifecycle?]")
    st.markdown("Yes. This document will serve as part of that.")
    st.markdown(":violet[2. Did you put in place measures to continuously assess the quality of the input data to the AI system?]")
    st.markdown("No. Data that is used needs to be assessed beforehand.")
    st.markdown(":violet[3. Can you trace back which data was used by the AI system to make a certain decision(s) or recommendation(s)?]")
    st.markdown("Yes. Since you know exactly what info you’re feeding the model. You can trace back what information it used to make the decisions.")
    st.markdown(":violet[4. Can you trace back which AI model or rules led to the decision(s) or recommendation(s) of the AI system?] ")
    st.markdown("Yes. Through the same method.")
    st.markdown(":violet[5. Did you put in place measures to continuously assess the quality of the output(s) of the AI system?")
    st.markdown("Yes")
    st.markdown(":violet[6. Did you put in place measures to continuously assess the quality of the output(s) of the AI system?]")
    st.markdown("Yes")
    st.markdown(":violet[7. Did you put adequate logging practices in place to record the decision(s) or recommendation(s) of the AI system?]")
    st.markdown("Yes")
    


with tab7:
    st.subheader("*:violet[REQUIREMENT #5 - Diversity, Non-Discrimination, And Fairness]*")
    st.markdown("*:green[Avoidance of Unfair Bias]*")
    st.markdown(":red[1. Did you establish a strategy or a set of procedures to avoid creating or reinforcing unfair bias in the AI system, both regarding the use of input data as well as for the algorithm design?]")
    st.markdown("Yes. We are following the ethical guidelines mentioned in the GDPR framework.")
    st.markdown(":red[2. Did you consider the diversity and representativeness of end-users and/or subjects in the data?]")
    st.markdown("We are working on the crime statistics related to different Breda neighborhoods for our project. The average annual income of Breda residents is also shown. Therefore, the likelihood of bias or discrimination in these data is extremely low. So I can confirm that sure, we have thought about the effects of our data not being sufficiently representative.")
    st.markdown(":orange[* Did you test for specific target groups or problematic use cases?]")
    st.markdown("No. It was not necessary.")
    st.markdown(":orange[* Did you research and use publicly available technical tools, that are state-of-the-art, to improve your understanding of the data, model, and performance?]")
    st.markdown("Yes")
    st.markdown(":red[3. Did you put in place educational and awareness initiatives to help AI designers and AI developers be more aware of the possible bias they can inject in designing and developing the AI system?]")
    st.markdown("Yes. We first ensured that everyone knew what each other was doing and thus followed all the right practices and educational and awareness initiatives. We filled the DEDA framework to ensure that we were ethical and considerate")
    st.markdown(":red[4. Did you ensure a mechanism that allows for the flagging of issues related to bias, discrimination, or poor performance of the AI system?]F")
    st.markdown("Yes. We have given different members of the group responsibility for detecting bias, discrimination, or poor performance.")
    st.markdown(":orange[* Did you establish clear steps and ways of communicating on how and to whom such issues can be raised?]")
    st.markdown("Yes. We made sure to communicate all the requirements when it comes to ethics at the beginning and there is also constant checking for this as well.]")
    st.markdown(":orange[* Did you identify the subjects that could potentially be (in)directly affected by the AI system, in addition to the (end-)users and/or subjects?]")
    st.markdown("Yes, and we also talked about the consequences that the people who will be affected by this will face i.e. criminals. And since we are also taking the income and crime rates per neighborhood, we won't be focusing on any individual group/person.")
    st.markdown(":red[5. Is your definition of fairness commonly used and implemented in any phase of the process of setting up the AI system?]")
    st.markdown("Yes. We are following different ethical guidelines to ensure this as well. It was implemented during the process of collecting and cleaning the data.")
    st.markdown(":orange[* Did you consider other definitions of fairness before choosing this one?]")
    st.markdown("No. This was something I always intended on using.")
    st.markdown(":red[6. Did you ensure a quantitative analysis or metrics to measure and test the applied definition of fairness?]")
    st.markdown("We had considered this, however, the data set does not contain the vulnerable data and hence fairness metrics are not required in our case.")
    st.markdown("------------")
    st.subheader("*:green[Accessibility and Universal Design]*")
    st.markdown(":red[1. Did you ensure that the AI system corresponds to the variety of preferences and abilities in society?")
    st.markdown("The project our group is working on focuses on the crime rates per neighborhood and we also take into consideration other factors such as education and income. Since this is mainly being worked on to improve Breda, it is quite evident that the AI system corresponds to preferences in society. What we are building is just a basic idea that the municipality can improve based on their needs. We are also constantly incorporating user feedback into the development process to help identify areas where the AI system may not adequately correspond to the preferences and abilities of users.")
    st.markdown(":red[2. Did you assess whether the AI system's user interface is usable by those with special needs or disabilities or those at risk of exclusion?]")
    st.markdown("This interface is not accessible to citizens. It is going to be used by the municipality to improve the quality of life of the citizens of Breda.")
    st.markdown(":orange[* Did you ensure that information about, and the AI system's user interface of, the AI system is accessible and usable also to users of assistive technologies (such as screen readers)?")
    st.markdown("No. Since it is going to be used by the municipality to improve the quality of life of the citizens of Breda.")
    st.markdown(":orange[* Did you involve or consult with end-users or subjects in need for assistive technology during the planning and development phase of the AI system?]")
    st.markdown("It was never mentioned during our consults with the end users.")
    st.markdown(":red[3. Did you ensure that Universal Design principles are taken into account during every step of the planning and development process, if applicable?]")
    st.markdown("This is applicable only to a very small extent because our final project is going to be a website and it is often very easy to use.")
    st.markdown(":red[4. Did you take the impact of the AI system on the potential end-users and/or subjects into account?]")
    st.markdown("The primary focus of the project is to ensure that consequences are appropriately applied to individuals who engage in criminal activities. The project aims to maintain a professional approach by adhering to legal frameworks, due process, and the principles of justice. The objective is to hold criminals accountable for their actions through lawful means, promoting a safe and secure environment for all members of the community. ")
    st.markdown(":orange[* Did you assess whether the team involved in building the AI system engaged with the possible target end-users and/or subjects?]")
    st.markdown("We maintain ongoing consultations with the end users to ensure their active participation and engagement in the project. By involving them in the process, we aim to enhance their understanding of the potential outcomes and gather valuable insights into their specific needs and requirements. These consultations serve as a means to foster effective collaboration, promote transparency, and align our efforts with the expectations and aspirations of the end users")
    st.markdown(":orange[* Did you assess whether there could be groups who might be disproportionately affected by the outcomes of the AI system?]")
    st.markdown("Yes. There aren't any groups.")
    st.markdown(":orange[* Did you assess the risk of the possible unfairness of the system onto the end-users or subject's communities?]")
    st.markdown("Yes.")
    st.markdown("------------")
    st.subheader("*:green[ Stakeholder Participation]*")
    st.markdown(":red[1. Did you consider a mechanism to include the participation of the widest range of possible stakeholders in the AI system’s design and development?]")
    st.markdown("No. We have not come up with a specific mechanism to include the participation of the stakeholders. However, we have frequent meetings with the involved parties to ensure that the progress of the group is up to their expectations.")
    
    
    
    
    
    
with tab6:
    st.subheader("*:violet[REQUIREMENT #6 Societal And Environmental Well-Being]*")
    st.markdown("*:green[Environmental Well-being]*")
    st.markdown(":red[1. Are there potential negative impacts of the AI system on the environment? Which potential impact(s) do you identify?]")
    st.markdown("There can be some negative impacts on the environment when it comes to the AI model such as increased energy consumption, environmental consequences of data center infrastructure, e-waste from improper disposal of AI hardware, biases in decision-making leading to discriminatory environmental outcomes, etc.")
    st.markdown(":red[2. Where possible, did you establish mechanisms to evaluate the environmental impact of the AI system’s development, deployment, and/or use (for example, the amount of energy used and carbon emissions)?]")
    st.markdown(":orange[* Did you define measures to reduce the environmental impact of the AI system throughout its lifecycle?")
    st.markdown("No. Mainly because what we are doing right now particularly does not produce a lot of environmental impacts")
    st.markdown("------------")
    st.subheader("*:green[Impact on Work and Skills]*")
    st.markdown(":red[1. Does the AI system impact human work and work arrangements?]")
    st.markdown("Yes. It may change certain work arrangements because the algorithm might try to predict areas with higher crimes and hence the municipality might have to make better arrangements to take care of that.")
    st.markdown(":orange[Did you pave the way for the introduction of the AI system in your organization by informing and consulting with impacted workers and their representatives (trade unions, (European) work councils) in advance?")
    st.markdown("No, it didn't seem necessary.")
    st.markdown(":red[2. Did you adopt measures to ensure that the impacts of the AI system on human work are well understood?]")
    st.markdown("Yes. By doing the DEDA framework and completing this checklist we are getting a better idea of the ethical considerations when it comes to the project.")
    st.markdown("*:orange[ Did you ensure that workers understand how the AI system operates, and which capabilities it has and which it does not have?]")
    st.markdown("Yes")
    st.markdown(":red[3. Could the AI system create the risk of deskilling the workforce?")
    st.markdown("No one is responsible for the task or outcome of our model.")
    st.markdown(":red[4. Does the system promote or require new (digital) skills? Did you provide training opportunities and materials for re- and up-skilling?]")
    st.markdown("No, the model does not require new digital skills.")
    st.markdown("------------")
    st.markdown("*:green[Impact on Society at Large or Democracy*")
    st.markdown(":red[1. Could the AI system have a negative impact on society at large or democracy?")
    st.markdown("No. It could not have a negative impact. In fact, it can only have a positive impact since our project aims on reducing crime rates in different neighborhoods of Breda.")
    st.markdown(":orange[* Did you assess the societal impact of the AI system’s use beyond the (end-)user and subject, such as potentially indirectly affecting stakeholders or society at large?]")
    st.markdown("Yes. The only people this project is going to affect are those committing crimes.")
    st.markdown(":orange[* Did you take action to minimize the potential societal harm of the AI system?]")
    st.markdown("No, as this is not applicable to us.")


with tab8:
    st.subheader("*:violet[REQUIREMENT #7 Accountability]*")
    st.subheader("*:green[ Auditability]*")
    st.markdown(":blue[1. Did you establish mechanisms that facilitate the AI system’s Auditability (e.g. tractability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?]")
    st.markdown("Yes. We have ensured to make a report on everything. The product underwent various stages, including conceptualization, design, and development. The data for the report was gathered from open-source data provided by the Gemeente. We had weekly consultations with the people involved in the project.")
    st.markdown(":blue[2. Did you ensure that the AI system can be audited by independent third parties?]")
    st.markdown("Yes, since this project is a part of our university curriculum, we have ensured to document everything and make everything easy for understanding.")
    st.markdown("------------")
    st.subheader("*:green[Risk Management]*")
    st.markdown(":blue[1. Did you foresee any kind of external guidance or third-party auditing processes to oversee ethical concerns and accountability measures?]")
    st.markdown("No. Not yet. We are still in the process of it. However, our mentor will make sure that we have followed all the ethical guidelines.")
    st.markdown(":violet[* Does the involvement of these third parties go beyond the development phase?]")
    st.markdown("Yes")
    st.markdown(":blue[2. Did you organize risk training and, if so, does this also inform about the potential legal framework applicable to the AI system?]")
    st.markdown("The DEDA framework covers data management, ethics, design considerations, and regular audits, while the ALTAI framework emphasizes accountability, liability, transparency, auditability, and interpretability. These frameworks serve as comprehensive methodologies that guide our ethical decision-making throughout the product lifecycle, promoting trust and integrity in our operations.")
    st.markdown(":blue[3. Did you consider establishing an AI ethics review board or a similar mechanism to discuss the overall accountability and ethical practices, including potential unclear grey areas?]")
    st.markdown("Yes")
    st.markdown(":blue[4. Did you establish a process to discuss and continuously monitor and assess the AI system's adherence to this Assessment List for Trustworthy AI (ALTAI)?]")
    st.markdown("Yes. We are ensuring this by following the Altai handbook and completing all the checklists.")
    st.markdown(":violet[* Does this process include identification and documentation of conflicts between the 6 aforementioned requirements or between different ethical principles and explanation of the 'trade-off' decisions made?]")
    st.markdown("Yes. We have made sure to complete all of the aforementioned requirements.")
    st.markdown(":blue[5. For applications that can adversely affect individuals, have redress by design mechanisms been put in place?]")
    st.markdown("Yes, redress-by-design feedback mechanisms have been diligently implemented for applications that can potentially have adverse effects on individuals.")


    
